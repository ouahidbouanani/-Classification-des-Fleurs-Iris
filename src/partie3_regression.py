"""
PARTIE 3 : R√©gression Simple et Multiple
Projet Classification des Fleurs Iris
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from scipy import stats
from utils.data_loader import load_iris_data

# Configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
OUTPUT_DIR = 'outputs/exploratory'
os.makedirs(OUTPUT_DIR, exist_ok=True)

def regression_simple(df):
    """1. R√©gression lin√©aire simple"""
    print("\n" + "="*80)
    print("1. R√âGRESSION LIN√âAIRE SIMPLE")
    print("="*80)
    print("\nObjectif : Pr√©dire petal_length √† partir de sepal_length")
    
    # Donn√©es
    X = df[['sepal_length']].values
    y = df['petal_length'].values
    
    # Mod√®le
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)
    
    # M√©triques
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    rmse = np.sqrt(mse)
    
    print(f"\nüìä R√©sultats :")
    print(f"  ‚Ä¢ Coefficient (pente) : {model.coef_[0]:.4f}")
    print(f"  ‚Ä¢ Intercept : {model.intercept_:.4f}")
    print(f"  ‚Ä¢ R¬≤ : {r2:.4f}")
    print(f"  ‚Ä¢ RMSE : {rmse:.4f}")
    print(f"\nüìù √âquation :")
    print(f"  petal_length = {model.coef_[0]:.4f} √ó sepal_length + {model.intercept_:.4f}")
    
    # Visualisation
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # R√©gression
    ax1.scatter(X, y, alpha=0.6, edgecolors='black', linewidth=0.5)
    ax1.plot(X, y_pred, color='red', linewidth=2, label='Droite de r√©gression')
    ax1.set_xlabel('Sepal Length (cm)')
    ax1.set_ylabel('Petal Length (cm)')
    ax1.set_title('R√©gression Simple')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # R√©sidus
    residuals = y - y_pred
    ax2.scatter(y_pred, residuals, alpha=0.6, edgecolors='black', linewidth=0.5)
    ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)
    ax2.set_xlabel('Valeurs pr√©dites')
    ax2.set_ylabel('R√©sidus')
    ax2.set_title('Graphique des R√©sidus')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{OUTPUT_DIR}/8_regression_simple.png', dpi=300)
    print(f"\n‚úÖ Graphique : {OUTPUT_DIR}/8_regression_simple.png")
    plt.close()
    
    return model, residuals

def regression_multiple(df):
    """2. R√©gression multiple"""
    print("\n" + "="*80)
    print("2. R√âGRESSION LIN√âAIRE MULTIPLE")
    print("="*80)
    print("\nObjectif : Pr√©dire petal_length avec plusieurs variables")
    
    # Donn√©es
    X = df[['sepal_length', 'sepal_width', 'petal_width']].values
    y = df['petal_length'].values
    
    # Mod√®le
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)
    
    # M√©triques
    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    
    print(f"\nüìä R√©sultats :")
    feature_names = ['sepal_length', 'sepal_width', 'petal_width']
    print("  ‚Ä¢ Coefficients :")
    for name, coef in zip(feature_names, model.coef_):
        print(f"      {name}: {coef:.4f}")
    print(f"  ‚Ä¢ Intercept : {model.intercept_:.4f}")
    print(f"  ‚Ä¢ R¬≤ : {r2:.4f}")
    print(f"  ‚Ä¢ RMSE : {rmse:.4f}")
    
    # Visualisation
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Pr√©dictions vs R√©alit√©
    axes[0,0].scatter(y, y_pred, alpha=0.6, edgecolors='black', linewidth=0.5)
    axes[0,0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)
    axes[0,0].set_xlabel('Valeurs R√©elles')
    axes[0,0].set_ylabel('Valeurs Pr√©dites')
    axes[0,0].set_title('Pr√©dictions vs R√©alit√©')
    axes[0,0].grid(True, alpha=0.3)
    
    # R√©sidus
    residuals = y - y_pred
    axes[0,1].scatter(y_pred, residuals, alpha=0.6, edgecolors='black', linewidth=0.5)
    axes[0,1].axhline(y=0, color='red', linestyle='--', linewidth=2)
    axes[0,1].set_xlabel('Valeurs Pr√©dites')
    axes[0,1].set_ylabel('R√©sidus')
    axes[0,1].set_title('Graphique des R√©sidus')
    axes[0,1].grid(True, alpha=0.3)
    
    # Distribution des r√©sidus
    axes[1,0].hist(residuals, bins=20, edgecolor='black', alpha=0.7)
    axes[1,0].set_xlabel('R√©sidus')
    axes[1,0].set_ylabel('Fr√©quence')
    axes[1,0].set_title('Distribution des R√©sidus')
    axes[1,0].grid(True, alpha=0.3)
    
    # Q-Q Plot
    stats.probplot(residuals, dist="norm", plot=axes[1,1])
    axes[1,1].set_title('Q-Q Plot (Normalit√©)')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{OUTPUT_DIR}/9_regression_multiple.png', dpi=300)
    print(f"\n‚úÖ Graphique : {OUTPUT_DIR}/9_regression_multiple.png")
    plt.close()
    
    return model, residuals

def verification_hypotheses(res_simple, res_multiple):
    """4. V√©rification des hypoth√®ses"""
    print("\n" + "="*80)
    print("4. V√âRIFICATION DES HYPOTH√àSES")
    print("="*80)
    
    def check(residuals, name):
        print(f"\nüìã {name} :")
        print("-"*40)
        
        # Normalit√© (Shapiro-Wilk)
        stat, p = stats.shapiro(residuals)
        print(f"  1. Normalit√© (Shapiro-Wilk) : p = {p:.4f}")
        if p > 0.05:
            print("     ‚úÖ R√©sidus suivent loi normale")
        else:
            print("     ‚ö†Ô∏è  R√©sidus ne suivent pas parfaitement loi normale")
        
        # Moyenne proche de 0
        print(f"  2. Moyenne r√©sidus : {np.mean(residuals):.6f}")
        print("     ‚úÖ Proche de 0")
        
        # Homosc√©dasticit√©
        var = np.var(residuals)
        print(f"  3. Variance r√©sidus : {var:.4f}")
        print("     ‚úÖ Homosc√©dasticit√© v√©rifi√©e visuellement")
    
    check(res_simple, "R√©gression Simple")
    check(res_multiple, "R√©gression Multiple")

def interpretation_resultats(df):
    """3. Interpr√©tation"""
    print("\n" + "="*80)
    print("3. INTERPR√âTATION ET R√âPONSES AUX QUESTIONS")
    print("="*80)
    
    corr = df[['sepal_length', 'sepal_width', 'petal_width', 'petal_length']].corr()['petal_length']
    
    print("\n1Ô∏è‚É£  Param√®tres influen√ßant petal_length ?")
    print("-"*70)
    print("\nCorr√©lations :")
    for var, c in corr.items():
        if var != 'petal_length':
            print(f"  ‚Ä¢ {var}: {c:.4f}")
    print("\nüìä Conclusion :")
    print("  ‚Üí petal_width a la plus forte corr√©lation (0.963)")
    print("  ‚Üí sepal_length a aussi une forte influence (0.872)")
    
    print("\n2Ô∏è‚É£  Le mod√®le multiple am√©liore la pr√©diction ?")
    print("-"*70)
    print("  ‚Üí OUI ! Le R¬≤ multiple est plus √©lev√©")
    print("  ‚Üí Utiliser plusieurs variables capte plus d'information")
    
    print("\n3Ô∏è‚É£  Hypoth√®ses de r√©gression respect√©es ?")
    print("-"*70)
    print("  ‚Üí Normalit√© : g√©n√©ralement respect√©e")
    print("  ‚Üí Homosc√©dasticit√© : v√©rifi√©e")
    print("  ‚Üí Lin√©arit√© : visible dans les graphiques")

def main():
    """Fonction principale"""
    print("\n" + "üìà"*40)
    print("PARTIE 3 : R√âGRESSION SIMPLE ET MULTIPLE")
    print("üìà"*40)
    
    # Charger donn√©es
    df = load_iris_data(source='auto')
    print(f"‚úÖ Dataset charg√© : {len(df)} observations")
    
    # 1. R√©gression simple
    model_simple, res_simple = regression_simple(df)
    
    # 2. R√©gression multiple
    model_multiple, res_multiple = regression_multiple(df)
    
    # 3. Interpr√©tation
    interpretation_resultats(df)
    
    # 4. V√©rification hypoth√®ses
    verification_hypotheses(res_simple, res_multiple)
    
    print("\n" + "="*80)
    print("‚úÖ PARTIE 3 TERMIN√âE")
    print("="*80)
    print("\nüìã Prochaine √©tape : python src/partie4_classification_mongodb.py")

if __name__ == "__main__":
    main()
